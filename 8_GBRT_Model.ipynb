{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GBRT Model: Gradient Boosting Regression Tree\n",
        "\n",
        "Gradient boosting fits residuals by sequentially adding weak learners (decision trees):\n",
        "\n",
        "- **Boosting**: Each tree fits the residuals of the previous tree\n",
        "- **Gradient Descent**: Uses gradient descent to optimize the loss function\n",
        "- **Strong Non-linear Modeling**: Usually performs better than Random Forest\n",
        "\n",
        "Main Hyperparameters:\n",
        "- `n_estimators`: Number of trees\n",
        "- `learning_rate`: Learning rate (shrinkage)\n",
        "- `max_depth`: Maximum depth of trees\n",
        "- `subsample`: Proportion of samples used by each tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Import utility functions\n",
        "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
        "from utils import (\n",
        "    load_data, \n",
        "    prepare_features_target, \n",
        "    calculate_r2_os, \n",
        "    build_portfolio_returns,\n",
        "    calculate_prediction_metrics\n",
        ")\n",
        "from TimeBasedCV import TimeBasedCV\n",
        "\n",
        "# Settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (604564, 23)\n",
            "Date range: 2019-01-01 00:00:00 to 2024-11-01 00:00:00\n",
            "Number of stocks: 13176\n",
            "\n",
            "Number of features: 16\n",
            "Number of samples: 604564\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = load_data('ger_factor_data_from2003.csv')\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Date range: {df['eom'].min()} to {df['eom'].max()}\")\n",
        "print(f\"Number of stocks: {df['id'].nunique()}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X, y, metadata, feature_names = prepare_features_target(df)\n",
        "print(f\"\\nNumber of features: {len(feature_names)}\")\n",
        "print(f\"Number of samples: {len(X)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup Time Series Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time span: 71 months (2019-01-01 to 2024-11-01)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train period: 2019-07-01 - 2021-07-01 ,val period: 2021-07-01 - 2022-01-01 , Test period 2022-01-01 - 2022-07-01 # train records 184419 ,# val records 53091 , # test records 55458\n",
            "Train period: 2019-10-01 - 2021-10-01 ,val period: 2021-10-01 - 2022-04-01 , Test period 2022-04-01 - 2022-10-01 # train records 188377 ,# val records 54619 , # test records 55511\n",
            "Train period: 2020-01-01 - 2022-01-01 ,val period: 2022-01-01 - 2022-07-01 , Test period 2022-07-01 - 2023-01-01 # train records 193202 ,# val records 55458 , # test records 55493\n",
            "Train period: 2020-04-01 - 2022-04-01 ,val period: 2022-04-01 - 2022-10-01 , Test period 2022-10-01 - 2023-04-01 # train records 198473 ,# val records 55511 , # test records 55300\n",
            "Train period: 2020-07-01 - 2022-07-01 ,val period: 2022-07-01 - 2023-01-01 , Test period 2023-01-01 - 2023-07-01 # train records 203973 ,# val records 55493 , # test records 54766\n",
            "Train period: 2020-10-01 - 2022-10-01 ,val period: 2022-10-01 - 2023-04-01 , Test period 2023-04-01 - 2023-10-01 # train records 209002 ,# val records 55300 , # test records 54621\n",
            "Train period: 2021-01-01 - 2023-01-01 ,val period: 2023-01-01 - 2023-07-01 , Test period 2023-07-01 - 2024-01-01 # train records 213534 ,# val records 54766 , # test records 54879\n",
            "Train period: 2021-04-01 - 2023-04-01 ,val period: 2023-04-01 - 2023-10-01 , Test period 2023-10-01 - 2024-04-01 # train records 216814 ,# val records 54621 , # test records 55227\n",
            "Train period: 2021-07-01 - 2023-07-01 ,val period: 2023-07-01 - 2024-01-01 , Test period 2024-01-01 - 2024-07-01 # train records 218808 ,# val records 54879 , # test records 55532\n",
            "Train period: 2021-10-01 - 2023-10-01 ,val period: 2023-10-01 - 2024-04-01 , Test period 2024-04-01 - 2024-10-01 # train records 220051 ,# val records 55227 , # test records 55985\n",
            "Generated 10 cross-validation folds\n"
          ]
        }
      ],
      "source": [
        "# Create time series cross-validation object\n",
        "# Adjust based on data length: data is about 6 years, use smaller train/val/test periods\n",
        "cv = TimeBasedCV(\n",
        "    train_period=24,   # 2 years training period\n",
        "    val_period=6,      # 6 months validation period\n",
        "    test_period=6,     # 6 months test period\n",
        "    freq='months'\n",
        ")\n",
        "\n",
        "# Prepare dataframe for CV\n",
        "cv_df = metadata.copy()\n",
        "cv_df['eom'] = pd.to_datetime(cv_df['eom'])\n",
        "\n",
        "# Set first split date\n",
        "min_date = cv_df['eom'].min()\n",
        "max_date = cv_df['eom'].max()\n",
        "total_months = (max_date.year - min_date.year) * 12 + (max_date.month - min_date.month) + 1\n",
        "print(f\"Total time span: {total_months} months ({min_date.date()} to {max_date.date()})\")\n",
        "\n",
        "first_split_date = (min_date + pd.DateOffset(months=30)).date()\n",
        "second_split_date = (min_date + pd.DateOffset(months=36)).date()\n",
        "\n",
        "# Generate cross-validation folds\n",
        "folds = cv.split(cv_df, first_split_date, second_split_date, date_column='eom', gap=0)\n",
        "\n",
        "print(f\"Generated {len(folds)} cross-validation folds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Model Configuration (Fixed Parameters)\n",
        "\n",
        "**Strategy**: Use empirically stable configuration (Method A), skip hyperparameter search\n",
        "\n",
        "For asset pricing scenarios (large samples + high-dimensional features), use the following fixed configuration:\n",
        "- `n_estimators=300`: Number of trees\n",
        "- `learning_rate=0.05`: Learning rate (step size)\n",
        "- `max_depth=3`: Tree depth (weak learner, consistent with boosting theory)\n",
        "- `subsample=0.7`: Each tree uses 70% of samples (increases randomness, prevents overfitting)\n",
        "- `min_samples_leaf=50`: At least 50 samples in leaf nodes (prevents very narrow leaves in panel data)\n",
        "\n",
        "Characteristics of this configuration:\n",
        "- Trees are not deep (3 layers), each tree is \"weak\", more like the weak learners required by boosting theory\n",
        "- Increases randomness and prevents overfitting through subsample < 1\n",
        "- min_samples_leaf=50 prevents very narrow leaves in panel data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Using Method A: Fixed configuration (empirically stable parameters)\n",
            "\n",
            "Fixed parameter configuration:\n",
            "  n_estimators=300\n",
            "  learning_rate=0.05\n",
            "  max_depth=3\n",
            "  subsample=0.7\n",
            "  min_samples_leaf=50\n",
            "  random_state=42\n",
            "\n",
            "Note: Use these parameters directly for training, skip hyperparameter search, significantly speed up training\n"
          ]
        }
      ],
      "source": [
        "# Use fixed configuration (Method A: empirically stable parameters)\n",
        "FIXED_PARAMS = {\n",
        "    'n_estimators': 300,\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 3,\n",
        "    'subsample': 0.7,\n",
        "    'min_samples_leaf': 50,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "print(\"âœ… Using Method A: Fixed configuration (empirically stable parameters)\")\n",
        "print(\"\\nFixed parameter configuration:\")\n",
        "for key, value in FIXED_PARAMS.items():\n",
        "    print(f\"  {key}={value}\")\n",
        "print(\"\\nNote: Use these parameters directly for training, skip hyperparameter search, significantly speed up training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Model and Make Predictions (Using Fixed Configuration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing fold 1/10...\n",
            "  ðŸ“Œ Using fixed configuration (Method A: empirically stable parameters)\n",
            "  Training... (n_estimators=300, learning_rate=0.05, max_depth=3)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training... (n_estimators=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFIXED_PARAMS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, learning_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFIXED_PARAMS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFIXED_PARAMS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trainval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trainval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Save feature importances\u001b[39;00m\n\u001b[1;32m     45\u001b[0m feature_importances_list\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mfeature_importances_)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[1;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[1;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/tree/_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Store all prediction results\n",
        "all_predictions = []\n",
        "all_actuals = []\n",
        "all_dates = []\n",
        "all_ids = []\n",
        "feature_importances_list = []\n",
        "\n",
        "# Train and predict for each fold\n",
        "for fold_idx, (train_idx, val_idx, test_idx) in enumerate(folds):\n",
        "    print(f\"\\nProcessing fold {fold_idx + 1}/{len(folds)}...\")\n",
        "    print(f\"  ðŸ“Œ Using fixed configuration (Method A: empirically stable parameters)\")\n",
        "    \n",
        "    # Prepare training, validation, and test data\n",
        "    X_train = X.iloc[train_idx].values\n",
        "    y_train = y.iloc[train_idx].values\n",
        "    X_val = X.iloc[val_idx].values\n",
        "    y_val = y.iloc[val_idx].values\n",
        "    X_test = X.iloc[test_idx].values\n",
        "    y_test = y.iloc[test_idx].values\n",
        "    \n",
        "    # Standardize features (GBRT usually doesn't need this, but for consistency)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Use fixed configuration to train on training+validation set\n",
        "    X_trainval = np.vstack([X_train_scaled, X_val_scaled])\n",
        "    y_trainval = np.hstack([y_train, y_val])\n",
        "    \n",
        "    model = GradientBoostingRegressor(\n",
        "        n_estimators=FIXED_PARAMS['n_estimators'],\n",
        "        learning_rate=FIXED_PARAMS['learning_rate'],\n",
        "        max_depth=FIXED_PARAMS['max_depth'],\n",
        "        subsample=FIXED_PARAMS['subsample'],\n",
        "        min_samples_leaf=FIXED_PARAMS['min_samples_leaf'],\n",
        "        random_state=FIXED_PARAMS['random_state']\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    print(f\"  Training... (n_estimators={FIXED_PARAMS['n_estimators']}, learning_rate={FIXED_PARAMS['learning_rate']}, max_depth={FIXED_PARAMS['max_depth']})\")\n",
        "    model.fit(X_trainval, y_trainval)\n",
        "    \n",
        "    # Save feature importances\n",
        "    feature_importances_list.append(model.feature_importances_)\n",
        "    \n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Save results\n",
        "    all_predictions.extend(y_pred)\n",
        "    all_actuals.extend(y_test)\n",
        "    all_dates.extend(metadata.iloc[test_idx]['eom'].values)\n",
        "    all_ids.extend(metadata.iloc[test_idx]['id'].values)\n",
        "    \n",
        "    # Print fold performance\n",
        "    r2_fold = calculate_r2_os(y_test, y_pred)\n",
        "    print(f\"  ðŸ“ˆ Test RÂ²_OS: {r2_fold:.4f}\")\n",
        "\n",
        "print(f\"\\nâœ… Completed predictions for all folds!\")\n",
        "print(f\"Total prediction samples: {len(all_predictions)}\")\n",
        "print(f\"\\nFixed parameter configuration used:\")\n",
        "for key, value in FIXED_PARAMS.items():\n",
        "    print(f\"  {key}={value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall performance metrics\n",
        "metrics = calculate_prediction_metrics(all_actuals, all_predictions)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"GBRT Model Prediction Performance Metrics\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in metrics.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key:20s}: {value:8.4f}\")\n",
        "    else:\n",
        "        print(f\"{key:20s}: {value}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate average feature importance\n",
        "avg_feature_importance = np.mean(feature_importances_list, axis=0)\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': avg_feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 Most Important Features:\")\n",
        "print(feature_importance_df.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Portfolio and Calculate Returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build portfolio returns\n",
        "portfolio_df, summary_stats = build_portfolio_returns(\n",
        "    all_actuals, \n",
        "    all_predictions, \n",
        "    all_dates, \n",
        "    all_ids, \n",
        "    n_deciles=10\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Long-Short Portfolio Performance (GBRT Model)\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in summary_stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key:25s}: {value:10.4f}\")\n",
        "    else:\n",
        "        print(f\"{key:25s}: {value}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Cumulative return curve\n",
        "if 'long_short' in portfolio_df.columns:\n",
        "    portfolio_df['cumulative_return'] = (1 + portfolio_df['long_short']).cumprod() - 1\n",
        "    \n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(portfolio_df['date'], portfolio_df['cumulative_return'], linewidth=2, color='purple')\n",
        "    plt.title('GBRT Model: Long-Short Portfolio Cumulative Returns', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Cumulative Return', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 2. Predicted vs Actual scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(all_actuals, all_predictions, alpha=0.1, s=1)\n",
        "plt.plot([np.min(all_actuals), np.max(all_actuals)], \n",
        "         [np.min(all_actuals), np.max(all_actuals)], \n",
        "         'r--', linewidth=2, label='Perfect Prediction Line')\n",
        "plt.xlabel('Actual Returns', fontsize=12)\n",
        "plt.ylabel('Predicted Returns', fontsize=12)\n",
        "plt.title(f'GBRT Model: Predicted vs Actual\\nRÂ²_OS = {metrics[\"r2_os\"]:.4f}', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Feature importance visualization (top 20)\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = feature_importance_df.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'], color='purple')\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance', fontsize=12)\n",
        "plt.title('GBRT Model: Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
